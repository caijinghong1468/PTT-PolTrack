{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a6a7545-1204-4e3f-a3c5-2d5d19369cb4",
   "metadata": {},
   "source": [
    "# **ü¶πüèºModelË®ìÁ∑¥**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008b3b82-a6d5-4643-96ef-291d2452a5f6",
   "metadata": {},
   "source": [
    "### üß∞ ÂÆâË£ùÂ∑•ÂÖ∑ÁÆ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2b2c0ecf-031e-4327-abbf-0beb1a1f5a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ë≥áÊñôÂâçËôïÁêÜ\n",
    "import os\n",
    "from os import path\n",
    "import jieba\n",
    "from sklearn.model_selection import train_test_split # Áî®‰æÜÂàáÂâ≤Ë≥áÊñôÈõÜ\n",
    "from sklearn.preprocessing import LabelEncoder # Áî®‰æÜÂ∞áÊ®ôÁ±§ËΩâÊèõÊàêÊ©üÂô®ÁúãÂæóÊáÇÁöÑÊ†ºÂºè\n",
    "from sklearn import tree # ÂæûsklearnÁöÑÊ®°ÂûãÈÅ∏ÊìáÂ∑•ÂÖ∑ÁÆ±Ë£°Èù¢ÊääÁî®‰æÜË®ìÁ∑¥Ê®°ÂûãÁöÑÂ∑•ÂÖ∑ÊãøÂá∫‰æÜ\n",
    "import pandas as pd # Áî®‰æÜÈñãÂïüË≥áÊñôÈõÜ\n",
    "import re # Áî®‰æÜÊäìÂèñÊàñËÄÖÊ∏ÖÁêÜË≥áÊñô‰∏≠ÁöÑË≥áË®ä\n",
    "# ÊäìÂèñÁâπÂæµ\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Ë®ìÁ∑¥Ê®°Âûã\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# Ë©ï‰º∞Ê®°Âûã\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "def text_length(data):\n",
    "  sentence_length=[]\n",
    "  for comment in data:\n",
    "    length=len(comment)\n",
    "    sentence_length.append(length)\n",
    "  return sentence_length\n",
    "\n",
    "def frequency(data,regex):\n",
    "  freq=[]\n",
    "  for string in data:\n",
    "      count = len(re.findall(regex, string))\n",
    "      freq.append(count)\n",
    "  return freq\n",
    "\n",
    "def boolean(data, regex):\n",
    "  boolean_list = []\n",
    "  for comment in data:\n",
    "    word_match = bool(re.search(regex, comment))\n",
    "    boolean_list.append(word_match)\n",
    "  return boolean_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0d6b16-0a37-44b9-aede-bb422ed452fa",
   "metadata": {},
   "source": [
    "### ü™ö ÈÄôÊòØÁî®‰æÜ**Êñ∑Ë©û**ÁöÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b8c1f29c-4a3f-4b9c-a730-9599034a89e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(text):\n",
    "  return jieba.lcut(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d590f5df-ca06-4953-9c28-6599e7715c6a",
   "metadata": {},
   "source": [
    "### üîç ÈÄôÊòØÁî®‰æÜ**ÊäìÂèñÁâπÂæµ**ÁöÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2aabbc15-0503-4f06-9b4f-8d82ce8a4361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extract(text, vectorizer):\n",
    "  # Â∞á list ÂÖÉÁ¥†ËΩâÊèõÁÇ∫Â≠ó‰∏≤\n",
    "  text = [' '.join(tokens) for tokens in text]  \n",
    "  matrix = vectorizer.fit_transform(text)\n",
    "  array = vectorizer.get_feature_names_out()\n",
    "  feature_df = pd.DataFrame(matrix.toarray(), columns = array)\n",
    "  return feature_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fb35cf-f51f-4297-9842-de1193475019",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è ÈÄôÊòØÁî®‰æÜ**Ë®ìÁ∑¥Ê®°Âûã**ÁöÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e01d7564-7840-4733-8a8b-a235445ad10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "  def __init__(self, feature, label, model):\n",
    "    self.feature = feature\n",
    "    self.label = label\n",
    "    self.model = model\n",
    "\n",
    "  # Â∞á data ÂàÜÁÇ∫ training set & test set\n",
    "  def split_data(self):\n",
    "    self.feat_train, self.feat_test, self.label_train, self.label_test = train_test_split(self.feature, self.label, test_size=0.2, random_state = 123) # test_size=0.2Ë°®Á§∫test set‰Ωî20%\n",
    "\n",
    "  # Áî®training set Ë®ìÁ∑¥Ê®°Âûã\n",
    "  def model_train(self):\n",
    "    self.model.fit(self.feat_train, self.label_train)\n",
    "\n",
    "  # Áî®Ë®ìÁ∑¥Â•ΩÁöÑÊ®°ÂûãÈ†êÊ∏¨ test set\n",
    "  def model_predict(self):\n",
    "    self.prediction = self.model.predict(self.feat_test)\n",
    "    print('predicted result: ' + str(self.prediction))\n",
    "\n",
    "  # Ë©ï‰º∞Ê®°ÂûãË°®Áèæ\n",
    "  def model_evaluate(self):\n",
    "    print(ConfusionMatrixDisplay.from_predictions(self.label_test, self.prediction, cmap = \"Blues\")) # Ê∑∑Ê∑ÜÁü©Èô£\n",
    "    evaluation = precision_recall_fscore_support(self.label_test, self.prediction, average='macro') # Ë®àÁÆó precision, recall, F-score\n",
    "    accuracy = accuracy_score(self.label_test, self.prediction) # Ë®àÁÆó accuracy\n",
    "    print(\"accuracy: \" + str(round(accuracy, 2)) + \"\\nprecision: \" + str(round(evaluation[0], 2)) + \"\\nrecall: \" + str(round(evaluation[1], 2)) + \"\\nfscore: \" + str(round(evaluation[2],2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3fba9a-48c5-4837-a45f-b7a991dba858",
   "metadata": {},
   "source": [
    " ### üßª‰ΩøÁî®jiebaÂÅöÊñ∑Ë©ûËàáÂà™Èô§stopwords\n",
    "   ##### Á®ãÂºèÂÖßÂÆπÔºö\n",
    " - ËºâÂÖ•ÂÅúÁî®Ë©ûÂíå‰ΩøÁî®ËÄÖÂ≠óÂÖ∏\n",
    " - ‰ΩøÁî®jiebaÊñ∑Ë©û‰∏¶ÁßªÈô§Êñ∑Ë©û‰∏≠ÁöÑÂÅúÁî®Ë©ûÔºåÊúÄÂæåÂ∞áËôïÁêÜÂæåÁöÑÁµêÊûúÂ≠òÂõûÂÖßÊñáÊ¨Ñ‰Ωç„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "29f171b0-3fca-4192-964f-13aa01438379",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'stopwords_zhTW.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m df_new \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m      3\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./src/jieba_clean_result.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m stopword \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstopwords_zhTW.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUTF-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m      5\u001b[0m stopword_list \u001b[38;5;241m=\u001b[39m stopword\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./src/original_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'stopwords_zhTW.txt'"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "df_new = pd.DataFrame()\n",
    "file = './src/jieba_clean_result.csv'\n",
    "stopword = open(\"stopwords_zhTW.txt\", \"r\", encoding='UTF-8').read()\n",
    "stopword_list = stopword.split(\"\\n\")\n",
    "df = pd.read_csv('./src/original_data.csv')\n",
    "new_word = open(\"./src/my_dict.txt\", \"r\", encoding='UTF-8').read()\n",
    "jieba.load_userdict(\"./src/my_dict.txt\")\n",
    "for text in df['ÂÖßÊñá']:\n",
    "  jieba_clean_result = []\n",
    "  jieba_result = jieba.lcut(text)\n",
    "  for token in jieba_result:\n",
    "    if token not in stopword_list:\n",
    "      jieba_clean_result.append(token)\n",
    "  output.append(jieba_clean_result)\n",
    "df_new['jieba_clean_result'] = output\n",
    "df_new.to_csv(file,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce49dcd6-4f67-40e3-b1ff-caaf7b553e43",
   "metadata": {},
   "source": [
    "### üìë TF-IDF\n",
    "‰∏ÄÊ®£Áî®`feature_extract()`ÁöÑÂäüËÉΩ\\\n",
    "`TfidfVectorizer()` ÊòØË®àÁÆó TF-IDF ÁöÑÂ∑•ÂÖ∑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0b4308-3686-4e4d-aa60-b5367b836653",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_feature = feature_extract(df_new['jieba_clean_result'], TfidfVectorizer(tokenizer = segment))\n",
    "tfidf_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033f3d03-b6a9-4ca6-b394-a3ab3378099d",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Ë®ìÁ∑¥Ê®°Âûã\n",
    "ÊàëÂÄë‰ΩøÁî®‰∫îÁ®ÆÂàÜÈ°ûÊ®°ÂûãÔºö\n",
    "- Ê±∫Á≠ñÊ®π Decision Tree\n",
    "- Èö®Ê©üÊ£ÆÊûó Random Forest\n",
    "- ÁæÖÂêâÊñØËø¥Ê≠∏ Logistic Regression\n",
    "- ÊîØÊè¥ÂêëÈáèÊ©ü Support Vector Machine\n",
    "- Â§öÈ†ÖÂºèÂûãÊ®∏Á¥†Ë≤ùËëâÊñØ Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17861c78-0f13-46fc-9fb0-c9634c65e738",
   "metadata": {},
   "outputs": [],
   "source": [
    "DT = DecisionTreeClassifier(criterion = 'entropy', max_depth = 5 , random_state = 123)\n",
    "RF = RandomForestClassifier(n_estimators = 120 , criterion = 'gini', max_depth = 10 , random_state = 123)\n",
    "LR = LogisticRegression(random_state = 1223)\n",
    "SVM = SVC(C=1, gamma='auto', kernel='linear', random_state = 123)\n",
    "NB = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf42075-083a-4951-805f-b60d6ec6de60",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6450a29-9318-4aca-8c85-bf4b0827a678",
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_model_bow = Model(tfidf_feature,df['label'], DT)  \n",
    "DT_model_bow.split_data()  \n",
    "DT_model_bow.model_train()  \n",
    "DT_model_bow.model_predict()  \n",
    "DT_model_bow.model_evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206b1eb9-fa7c-4a99-8bb1-1d5ecc0922de",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408010ad-7882-4932-bb02-5b2d03c12863",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_model_bow = Model(tfidf_feature,df['label'], RF)  \n",
    "RF_model_bow.split_data()  \n",
    "RF_model_bow.model_train()  \n",
    "RF_model_bow.model_predict()  \n",
    "RF_model_bow.model_evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed85e71-b9f9-43aa-bb81-c8105547d96b",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ac7dbf-2636-421e-9d9e-114a15a32625",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_model_bow = Model(tfidf_feature,df['label'], LR)  \n",
    "LR_model_bow.split_data()  \n",
    "LR_model_bow.model_train()  \n",
    "LR_model_bow.model_predict()  \n",
    "LR_model_bow.model_evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71df79b2-b079-4ce4-8c29-98254337295d",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f260eb-f440-4b0a-a799-a1c808a0bf62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SVM_model_bow = Model(tfidf_feature,df['label'], SVM)  \n",
    "SVM_model_bow.split_data()  \n",
    "SVM_model_bow.model_train()  \n",
    "SVM_model_bow.model_predict()  \n",
    "SVM_model_bow.model_evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cdcbd2-93af-4616-b36d-26b5981eadc3",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10be360e-4638-4b8b-89be-782e45893334",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_model_bow = Model(tfidf_feature,df['label'], NB)  \n",
    "NB_model_bow.split_data()  \n",
    "NB_model_bow.model_train()  \n",
    "NB_model_bow.model_predict()  \n",
    "NB_model_bow.model_evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
