{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc67259-d73d-4937-af65-4cb03065201b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65837e98-c42a-4a8a-a511-328527ba5fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   載入套件\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2029ea00-a5bd-4ae3-8b6b-9e182802fbdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2125a7a-cf7a-4c3e-b481-98dc30b2ebd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   建立函式\n",
    "def get_ptt_post(soup):\n",
    "\n",
    "    #   取得文章列表\n",
    "    #   目標 div\n",
    "    data = soup.select(\"div.r-ent\")\n",
    "    result = []\n",
    "    for i in data:\n",
    "\n",
    "        #   標題\n",
    "        title = i.select(\"div.title\")[0].text.strip()\n",
    "\n",
    "        #   忽略 公告類文章 & 已刪文文章\n",
    "        if \"公告\" in title or (\"已被\" in title and \"刪除\" in title):\n",
    "            continue\n",
    "\n",
    "        #   發文時間\n",
    "        date = i.select(\"div.date\")[0].text.strip()\n",
    "\n",
    "        #   作者\n",
    "        author = i.select(\"div.author\")[0].text.strip()\n",
    "\n",
    "        #   網址\n",
    "        #   PTT 網站\n",
    "        oriLink = \"https://www.ptt.cc\" + i.select(\"div.title a\")[0][\"href\"]\n",
    "\n",
    "        #   取得文章相關資訊\n",
    "        #   內文\n",
    "        #   請求文章內文\n",
    "        res_content = requests.get(oriLink)\n",
    "        soup_content = bs(res_content.text,\"lxml\")\n",
    "        #  文章內容簡易驗證\n",
    "        results_content = soup_content.select('span.article-meta-value')\n",
    "        if len(results_content) > 3:\n",
    "            #   驗證成功, 篩出文章內文\n",
    "            content = soup_content.find(id=\"main-content\").text\n",
    "            Footer = u'※ 發信站: 批踢踢實業坊(ptt.cc),'\n",
    "            #   移除註腳以下內容\n",
    "            content = content.split(Footer)\n",
    "            #   存取內容\n",
    "            main_content = content[0]\n",
    "            pass\n",
    "        else:\n",
    "            #   驗證失敗, 跳過該文章\n",
    "            print(oriLink,\"內文異常:ID/版標/標題/日期為空\")\n",
    "            print(results_content)\n",
    "            continue\n",
    "\n",
    "        result.append({\n",
    "            \"title\":title,\n",
    "            \"date\":date,\n",
    "            \"author\":author,\n",
    "            \"link\":oriLink,\n",
    "            \"content\":main_content\n",
    "        })\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aacd81b-c0ad-4e99-bbec-1ce0ef3cc1a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e0a96c2-4377-4682-8889-82711ed42e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.ptt.cc/bbs/HatePolitics/index4164.html is ok\n"
     ]
    }
   ],
   "source": [
    "#   前綴與首頁網址\n",
    "prefix = \"https://www.ptt.cc\"\n",
    "url = \"https://www.ptt.cc/bbs/HatePolitics/index.html\"\n",
    "\n",
    "#   請求與解析\n",
    "res = requests.get(url)\n",
    "soup = bs(res.text,\"lxml\")\n",
    "\n",
    "\n",
    "#   CH4 - 建立資料集並匯出\n",
    "#   首次呼叫 Def\n",
    "output = []\n",
    "result = get_ptt_post(soup)\n",
    "\n",
    "#   將結果韓式返回的內容存入output陣列\n",
    "output += result\n",
    "\n",
    "#   再次呼叫數(N)頁 - 前頁網址\n",
    "N = 1\n",
    "previous_page = soup.select(\"div#action-bar-container div.action-bar div.btn-group-paging a\")[1][\"href\"]\n",
    "# Extract the page number using split and indexing\n",
    "page_number = int(previous_page.split('/')[-1].split('.')[0].replace('index',''))\n",
    "\n",
    "for i in range(N):\n",
    "    url = \"https://www.ptt.cc/bbs/HatePolitics/index{}\".format(page_number-i)+\".html\"\n",
    "    res = requests.get(url)\n",
    "    soup = bs(res.text,\"lxml\")\n",
    "\n",
    "    result = get_ptt_post(soup)\n",
    "    output += result\n",
    "    print(\"{} is ok\".format(url))\n",
    "\n",
    "#   產出資料集\n",
    "df = pd.DataFrame(output)\n",
    "\n",
    "#   欄位重新命名\n",
    "df.rename(columns={\n",
    "    \"title\":'標題',\n",
    "    \"date\":'發文時間',\n",
    "    \"author\":'作者',\n",
    "    \"link\":'網址',\n",
    "    \"content\":'內文'\n",
    "    }, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821805f7-b0aa-4afe-8e1b-22230525515f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b32d835-b6ea-4f9e-a2d7-4b63a7996bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 假设需要处理的文本在某一列中，例如 'content' 列\n",
    "# 请确认您文件中实际的文本列名\n",
    "if '內文' in df.columns:\n",
    "    def process_text(text):\n",
    "        # Check if the input is a string\n",
    "        if isinstance(text, str):\n",
    "            # 正则表达式处理文本\n",
    "            pattern = r'標題(.*?)時間.*?\\n(.*)'\n",
    "            match = re.search(pattern, text, re.DOTALL)\n",
    "            if match:\n",
    "                title = match.group(1).strip()\n",
    "                content = match.group(2).strip()\n",
    "                return f'{title}\\n{content}'\n",
    "        return text  # 如果不匹配，返回原文本或者非字符串类型\n",
    "\n",
    "    # 应用正则表达式处理每一行\n",
    "    df['內文'] = df['內文'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeb775c-5d11-478b-8d2c-6c297d61666f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51f4f459-2781-462a-8cf7-b4f4ac2d8e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = []\n",
    "for text in df['內文']:\n",
    "  # Check if the value is a string before applying re.sub\n",
    "  if isinstance(text, str):\n",
    "    clean = re.sub(r'(\\n|\\t|\\r|[a-z]|[A-Z]|http\\S+)', r'', text)\n",
    "    clean_text.append(clean)\n",
    "  else:\n",
    "    # Handle non-string values (e.g., NaN)\n",
    "    clean_text.append('')  # Or any other appropriate value\n",
    "\n",
    "# 清理完的text存成新的一欄 data\n",
    "df['內文'] = clean_text\n",
    "df=df[['內文']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dea0f51-314b-4087-8b2b-4689598bbc31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "610a36f3-e008-4590-b734-d4c723e16694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已移除含有 '[新聞]' 的列，結果儲存至 clean_processed_ptt_tech_job_post.csv\n"
     ]
    }
   ],
   "source": [
    "output_file = 'clean_processed_ptt_tech_job_post.csv'\n",
    "# 檢查 'clean_text' 欄位是否存在\n",
    "if '內文' in df.columns:\n",
    "    # 篩選資料，移除含有 \"[新聞]\" 的列\n",
    "    filtered_df = df[~df['內文'].str.contains(r'\\[新聞\\]', na=False)]\n",
    "    # 儲存到新的CSV檔案\n",
    "    filtered_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"已移除含有 '[新聞]' 的列，結果儲存至 {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
