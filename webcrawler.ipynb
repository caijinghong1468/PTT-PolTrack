{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65837e98-c42a-4a8a-a511-328527ba5fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#   透過Python抓取PTT文章列表與內文 by Wen Chieh\n",
    "import re\n",
    "#   載入套件\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "#   建立函式\n",
    "def get_ptt_post(soup):\n",
    "\n",
    "    #   CH2 - 取得文章列表\n",
    "    #   目標 div\n",
    "    data = soup.select(\"div.r-ent\")\n",
    "    result = []\n",
    "    for i in data:\n",
    "\n",
    "        #   1 - 標題\n",
    "        title = i.select(\"div.title\")[0].text.strip()\n",
    "\n",
    "        #   忽略 公告類文章 & 已刪文文章\n",
    "        if \"公告\" in title or (\"已被\" in title and \"刪除\" in title):\n",
    "            continue\n",
    "\n",
    "        #   2 - 發文時間\n",
    "        date = i.select(\"div.date\")[0].text.strip()\n",
    "\n",
    "        #   3 - 作者\n",
    "        author = i.select(\"div.author\")[0].text.strip()\n",
    "\n",
    "        #   4 - 網址\n",
    "        #   PTT 網站\n",
    "        oriLink = \"https://www.ptt.cc\" + i.select(\"div.title a\")[0][\"href\"]\n",
    "\n",
    "        #   CH3 - 取得文章相關資訊\n",
    "        #   8 - 內文\n",
    "        #   8.1 - 請求文章內文\n",
    "        res_content = requests.get(oriLink)\n",
    "        soup_content = bs(res_content.text,\"lxml\")\n",
    "        #   8.2 - 文章內容簡易驗證\n",
    "        results_content = soup_content.select('span.article-meta-value')\n",
    "        if len(results_content) > 3:\n",
    "            #   8.2.1 - 驗證成功, 篩出文章內文\n",
    "            content = soup_content.find(id=\"main-content\").text\n",
    "            Footer = u'※ 發信站: 批踢踢實業坊(ptt.cc),'\n",
    "            #   8.3 - 移除註腳以下內容\n",
    "            content = content.split(Footer)\n",
    "            #   8.4 - 存取內容\n",
    "            main_content = content[0]\n",
    "            pass\n",
    "        else:\n",
    "            #   8.2.2 - 驗證失敗, 跳過該文章\n",
    "            print(oriLink,\"內文異常:ID/版標/標題/日期為空\")\n",
    "            print(results_content)\n",
    "            continue\n",
    "\n",
    "        result.append({\n",
    "            \"title\":title,\n",
    "            \"date\":date,\n",
    "            \"author\":author,\n",
    "            \"link\":oriLink,\n",
    "            \"content\":main_content\n",
    "        })\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0a96c2-4377-4682-8889-82711ed42e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   前綴與首頁網址\n",
    "prefix = \"https://www.ptt.cc\"\n",
    "url = \"https://www.ptt.cc/bbs/HatePolitics/index.html\"\n",
    "\n",
    "#   請求與解析\n",
    "res = requests.get(url)\n",
    "soup = bs(res.text,\"lxml\")\n",
    "\n",
    "\n",
    "#   CH4 - 建立資料集並匯出\n",
    "#   首次呼叫 Def\n",
    "output = []\n",
    "result = get_ptt_post(soup)\n",
    "\n",
    "#   將結果韓式返回的內容存入output陣列\n",
    "output += result\n",
    "\n",
    "#   再次呼叫數(N)頁 - 前頁網址\n",
    "N = 100\n",
    "previous_page = soup.select(\"div#action-bar-container div.action-bar div.btn-group-paging a\")[1][\"href\"]\n",
    "# Extract the page number using split and indexing\n",
    "page_number = int(previous_page.split('/')[-1].split('.')[0].replace('index',''))\n",
    "\n",
    "for i in range(N):\n",
    "    url = \"https://www.ptt.cc/bbs/HatePolitics/index{}\".format(page_number-i)+\".html\"\n",
    "    res = requests.get(url)\n",
    "    soup = bs(res.text,\"lxml\")\n",
    "\n",
    "    result = get_ptt_post(soup)\n",
    "    output += result\n",
    "    print(\"{} is ok\".format(url))\n",
    "\n",
    "#   產出資料集\n",
    "df = pd.DataFrame(output)\n",
    "\n",
    "#   欄位重新命名\n",
    "df.rename(columns={\n",
    "    \"title\":'標題',\n",
    "    \"date\":'發文時間',\n",
    "    \"author\":'作者',\n",
    "    \"link\":'網址',\n",
    "    \"content\":'內文'\n",
    "    }, inplace=True)\n",
    "\n",
    "df.to_excel(\"ptt_tech_job_post.xlsx\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b32d835-b6ea-4f9e-a2d7-4b63a7996bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 读取 CSV 文件\n",
    "file_path = 'original_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 假设需要处理的文本在某一列中，例如 'content' 列\n",
    "# 请确认您文件中实际的文本列名\n",
    "if '內文' in data.columns:\n",
    "    def process_text(text):\n",
    "        # Check if the input is a string\n",
    "        if isinstance(text, str):\n",
    "            # 正则表达式处理文本\n",
    "            pattern = r'標題(.*?)時間.*?\\n(.*)'\n",
    "            match = re.search(pattern, text, re.DOTALL)\n",
    "            if match:\n",
    "                title = match.group(1).strip()\n",
    "                content = match.group(2).strip()\n",
    "                return f'{title}\\n{content}'\n",
    "        return text  # 如果不匹配，返回原文本或者非字符串类型\n",
    "\n",
    "    # 应用正则表达式处理每一行\n",
    "    data['processed_content'] = data['內文'].apply(process_text)\n",
    "\n",
    "    # 保存为新文件\n",
    "    output_path = 'processed_ptt_tech_job_post.csv'\n",
    "    data.to_csv(output_path, index=False)\n",
    "    print(f\"处理完成，文件已保存至: {output_path}\")\n",
    "else:\n",
    "    print(\"未找到 'content' 列，请确认文件中的列名。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f4f459-2781-462a-8cf7-b4f4ac2d8e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = []\n",
    "for text in data['Column1']:\n",
    "  # Check if the value is a string before applying re.sub\n",
    "  if isinstance(text, str):\n",
    "    clean = re.sub(r'(\\n|\\t|\\r|[a-z]|[A-Z]|http\\S+)', r'', text)\n",
    "    clean_text.append(clean)\n",
    "  else:\n",
    "    # Handle non-string values (e.g., NaN)\n",
    "    clean_text.append('')  # Or any other appropriate value\n",
    "\n",
    "# 清理完的text存成新的一欄 clean_text\n",
    "data['clean_text'] = clean_text\n",
    "data['clean_text']\n",
    "# 刪除 '欄位名稱' 欄位\n",
    "data = data.drop('內文', axis=1)\n",
    "data = data.drop('processed_content', axis=1)\n",
    "output_path = 'clean_processed_ptt_tech_job_post.csv'\n",
    "data.to_csv(output_path, index=False)\n",
    "print(f\"处理完成，文件已保存至: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610a36f3-e008-4590-b734-d4c723e16694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 讀取CSV檔案\n",
    "input_file = 'original_data.csv'  # 替換成你的檔案名稱\n",
    "output_file = 'original_data.csv'  # 建議更改為不同的輸出檔案名稱，避免覆蓋原始檔案\n",
    "\n",
    "# 載入資料\n",
    "df = pd.read_csv(input_file ,on_bad_lines='skip')\n",
    "\n",
    "# 檢查 'clean_text' 欄位是否存在\n",
    "if 'Column1' in df.columns:\n",
    "    # 篩選資料，移除含有 \"[新聞]\" 的列\n",
    "    filtered_df = df[~df['Column1'].str.contains(r'\\[新聞\\]', na=False)]\n",
    "\n",
    "    # 儲存到新的CSV檔案\n",
    "    filtered_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"已移除含有 '[新聞]' 的列，結果儲存至 {output_file}\")\n",
    "else:\n",
    "    print(f\"錯誤: 在 '{input_file}' 檔案中找不到 'clean_text' 欄位。\")\n",
    "    print(\"請確認你是否已正確執行先前的資料清理步驟，並將結果儲存到此檔案。\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
